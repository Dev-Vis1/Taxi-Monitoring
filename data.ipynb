{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad2990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from geopy.distance import geodesic # Used for Haversine distance calculation\n",
    "\n",
    "print(\"Required libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f3bddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: c:\\Users\\sanac\\anaconda3\\envs\\big_data\n",
      "\n",
      "  added / updated specs:\n",
      "    - tabulate\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  tabulate           pkgs/main/win-64::tabulate-0.9.0-py312haa95532_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working... done\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7438a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data directory: C:\\Users\\sanac\\Downloads\\Big Data\\T-drive Taxi Trajectories\\release\\taxi_log_2008_by_id\n",
      "Preprocessed data will be saved to: C:\\Users\\sanac\\Downloads\\Big Data\\T-drive Taxi Trajectories\\release\\taxi_preprocessed_for_kafka\n",
      "Output directory ensured.\n",
      "\n",
      "Verification: data_directory 'C:\\Users\\sanac\\Downloads\\Big Data\\T-drive Taxi Trajectories\\release\\taxi_log_2008_by_id' exists.\n",
      "Verification: Found 10357 .txt files in source directory.\n",
      "Verification: Example source file: C:\\Users\\sanac\\Downloads\\Big Data\\T-drive Taxi Trajectories\\release\\taxi_log_2008_by_id\\1.txt\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "data_directory = \"C:\\\\Users\\\\sanac\\\\Downloads\\\\Big Data\\\\T-drive Taxi Trajectories\\\\release\\\\taxi_log_2008_by_id\"\n",
    "\n",
    "# Output directory for the PREPROCESSED files (ready for Kafka)\n",
    "# These files will contain calculated speed, distance, and outlier flags.\n",
    "preprocessed_output_dir = \"C:\\\\Users\\\\sanac\\\\Downloads\\\\Big Data\\\\T-drive Taxi Trajectories\\\\release\\\\taxi_preprocessed_for_kafka\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(preprocessed_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Original data directory: {data_directory}\")\n",
    "print(f\"Preprocessed data will be saved to: {preprocessed_output_dir}\")\n",
    "print(\"Output directory ensured.\")\n",
    "\n",
    "# --- Quick path verification ---\n",
    "if os.path.exists(data_directory):\n",
    "    print(f\"\\nVerification: data_directory '{data_directory}' exists.\")\n",
    "    test_files = glob.glob(os.path.join(data_directory, \"*.txt\"))\n",
    "    print(f\"Verification: Found {len(test_files)} .txt files in source directory.\")\n",
    "    if len(test_files) > 0:\n",
    "        print(f\"Verification: Example source file: {test_files[0]}\")\n",
    "    else:\n",
    "        print(\"WARNING: No .txt files found in the data_directory. Double-check your path and file extensions!\")\n",
    "else:\n",
    "    print(f\"ERROR: data_directory '{data_directory}' does NOT exist. Please correct the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4590c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing function 'preprocess_taxi_data_for_kafka' defined.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Function\n",
    "\n",
    "def preprocess_taxi_data_for_kafka(file_path):\n",
    "    \"\"\"\n",
    "    Loads, cleans, and enriches a single taxi's trajectory data for Kafka replay.\n",
    "    Calculates time difference, distance, and speed. Flags potential outliers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load Data\n",
    "        # Data structure: taxiId, timestamp, longitude, latitude\n",
    "        df = pd.read_csv(file_path, header=None, names=['taxi_id_raw', 'datetime_str', 'longitude', 'latitude'])\n",
    "\n",
    "        if df.empty:\n",
    "            return None, None # Return None if DataFrame is empty\n",
    "\n",
    "        # Extract taxi_id from filename (more reliable for consistent ID)\n",
    "        taxi_id = int(os.path.basename(file_path).replace('.txt', ''))\n",
    "        df['taxi_id'] = taxi_id\n",
    "\n",
    "        # 2. Parse Data Types\n",
    "        # Convert datetime string to datetime objects\n",
    "        df['datetime'] = pd.to_datetime(df['datetime_str'])\n",
    "\n",
    "        # 3. Sort Data by Timestamp (Crucial for sequential processing)\n",
    "        df = df.sort_values(by='datetime').reset_index(drop=True)\n",
    "\n",
    "        # 4. Remove Duplicate Consecutive Points (identical timestamp, longitude, latitude)\n",
    "        # These are redundant for real-time streaming and add no value.\n",
    "        df = df.drop_duplicates(subset=['datetime', 'longitude', 'latitude'], keep='first').reset_index(drop=True)\n",
    "\n",
    "        # 5. Calculate Time Difference (in minutes)\n",
    "        # Needed for speed calculation\n",
    "        df['prev_datetime'] = df['datetime'].shift(1)\n",
    "        df['time_diff'] = (df['datetime'] - df['prev_datetime']).dt.total_seconds() / 60 # In minutes\n",
    "\n",
    "        # 6. Calculate Distance (Haversine formula, in meters)\n",
    "        # This requires the previous latitude/longitude\n",
    "        df['prev_latitude'] = df['latitude'].shift(1)\n",
    "        df['prev_longitude'] = df['longitude'].shift(1)\n",
    "\n",
    "        # Function to apply Haversine distance\n",
    "        # Only calculate if both current and previous points are available\n",
    "        def calculate_haversine_distance(row):\n",
    "            if pd.isna(row['prev_latitude']) or pd.isna(row['prev_longitude']):\n",
    "                return np.nan\n",
    "            coords_1 = (row['prev_latitude'], row['prev_longitude'])\n",
    "            coords_2 = (row['latitude'], row['longitude'])\n",
    "            # geodesic returns distance in meters by default if unit is not specified,\n",
    "            # but Haversine is explicitly asked for. geopy.distance.haversine provides this.\n",
    "            return geodesic(coords_1, coords_2).meters # Returns distance in meters\n",
    "\n",
    "        df['distance'] = df.apply(calculate_haversine_distance, axis=1)\n",
    "\n",
    "        # 7. Calculate Speed (meters/second and km/hour)\n",
    "        # Speed = Distance / Time\n",
    "        df['speed_mps'] = df['distance'] / (df['time_diff'] * 60) # Convert time_diff from minutes to seconds\n",
    "        df['speed_kmph'] = df['speed_mps'] * 3.6 # Convert m/s to km/h\n",
    "\n",
    "        # Handle cases where time_diff is zero or near zero to avoid division by zero for speed.\n",
    "        # If time_diff is 0, speed is 0. If distance > 0 and time_diff <=0, it's an error.\n",
    "        df.loc[df['time_diff'] <= 0, 'speed_mps'] = 0\n",
    "        df.loc[df['time_diff'] <= 0, 'speed_kmph'] = 0\n",
    "\n",
    "\n",
    "        # 8. Identify Potential Outliers (Flagging)\n",
    "        # Based on common sense thresholds for urban taxi data\n",
    "        MAX_TIME_DIFF_MINUTES = 24 * 60 # 24 hours (for very large gaps)\n",
    "        MAX_SPEED_KMPH = 120 # Unrealistic speed for urban taxi\n",
    "        MIN_SPEED_KMPH_FOR_MOVEMENT = 0.5 # Effectively stationary\n",
    "        MAX_DISTANCE_METERS = 50000 # Teleportation error\n",
    "        SPEED_LIMIT_FOR_WARNING = 50 # km/h as per project description\n",
    "\n",
    "        df['is_outlier_time'] = (df['time_diff'] > MAX_TIME_DIFF_MINUTES) | (df['time_diff'] <= 0)\n",
    "        df['is_outlier_speed'] = (df['speed_kmph'] > MAX_SPEED_KMPH) | \\\n",
    "                                 ((df['speed_kmph'] < MIN_SPEED_KMPH_FOR_MOVEMENT) & (df['distance'] > 50)) # Low speed, high distance indicates GPS drift\n",
    "        df['is_outlier_distance'] = (df['distance'] > MAX_DISTANCE_METERS) # Teleportation\n",
    "\n",
    "        # Combine outlier flags. These points are problematic.\n",
    "        df['is_outlier'] = df['is_outlier_time'] | df['is_outlier_speed'] | df['is_outlier_distance']\n",
    "\n",
    "        # Add a flag for the specific speed limit mentioned for warnings\n",
    "        df['is_speeding'] = (df['speed_kmph'] > SPEED_LIMIT_FOR_WARNING) & (~df['is_outlier_speed']) # Only flag if not already an extreme outlier speed\n",
    "\n",
    "        # Handle the very first point of each trajectory:\n",
    "        # It has no previous point, so time_diff, distance, speed will be NaN.\n",
    "        # These NaNs will be present for the first row of each taxi in the cleaned data.\n",
    "        # For output, we can decide to keep them as NaN or fill. For Kafka, passing NaN is fine.\n",
    "\n",
    "        # Select and reorder columns for clarity in the output file\n",
    "        # We include datetime_str as well for easy Kafka payload creation\n",
    "        output_df = df[[\n",
    "            'taxi_id', 'datetime_str', 'longitude', 'latitude',\n",
    "            'time_diff', 'distance', 'speed_mps', 'speed_kmph',\n",
    "            'is_outlier_time', 'is_outlier_speed', 'is_outlier_distance', 'is_outlier', 'is_speeding'\n",
    "        ]].copy()\n",
    "\n",
    "        return output_df, None # We don't need a separate outliers_df as we're just flagging\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(file_path)}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "print(\"Preprocessing function 'preprocess_taxi_data_for_kafka' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120aa65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing for Kafka replay...\n",
      "Found 10357 taxi files in C:\\Users\\sanac\\Downloads\\Big Data\\T-drive Taxi Trajectories\\release\\taxi_log_2008_by_id.\n",
      "\n",
      "--- Preprocessing 10357 files for Kafka replay ---\n",
      "  Skipped empty or problematic file: 10115.txt\n",
      "  Skipped empty or problematic file: 10352.txt\n",
      "  Skipped empty or problematic file: 1089.txt\n",
      "  Processing file 500/10357: 1125.txt\n",
      "  Skipped empty or problematic file: 1497.txt\n",
      "  Processing file 1000/10357: 1576.txt\n",
      "  Skipped empty or problematic file: 1947.txt\n",
      "  Processing file 1500/10357: 2025.txt\n",
      "  Processing file 2000/10357: 2476.txt\n",
      "  Processing file 2500/10357: 2926.txt\n",
      "  Skipped empty or problematic file: 2929.txt\n",
      "  Skipped empty or problematic file: 2945.txt\n",
      "  Skipped empty or problematic file: 295.txt\n",
      "  Skipped empty or problematic file: 3050.txt\n",
      "  Skipped empty or problematic file: 3160.txt\n",
      "  Skipped empty or problematic file: 3194.txt\n",
      "  Processing file 3000/10357: 3376.txt\n",
      "  Processing file 3500/10357: 3826.txt\n",
      "  Skipped empty or problematic file: 3950.txt\n",
      "  Processing file 4000/10357: 4276.txt\n",
      "  Processing file 4500/10357: 4726.txt\n",
      "  Processing file 5000/10357: 5176.txt\n",
      "  Processing file 5500/10357: 5626.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanac\\anaconda3\\envs\\big_data\\Lib\\site-packages\\geopy\\point.py:472: UserWarning: Latitude normalization has been prohibited in the newer versions of geopy, because the normalized value happened to be on a different pole, which is probably not what was meant. If you pass coordinates as positional args, please make sure that the order is (latitude, longitude) or (y, x) in Cartesian terms.\n",
      "  return cls(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 5850.txt: Latitude must be in the [-90; 90] range.\n",
      "  Skipped empty or problematic file: 5850.txt\n",
      "  Skipped empty or problematic file: 5972.txt\n",
      "  Skipped empty or problematic file: 6030.txt\n",
      "  Processing file 6000/10357: 6076.txt\n",
      "  Skipped empty or problematic file: 6236.txt\n",
      "  Skipped empty or problematic file: 6322.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanac\\anaconda3\\envs\\big_data\\Lib\\site-packages\\geopy\\point.py:472: UserWarning: Latitude normalization has been prohibited in the newer versions of geopy, because the normalized value happened to be on a different pole, which is probably not what was meant. If you pass coordinates as positional args, please make sure that the order is (latitude, longitude) or (y, x) in Cartesian terms.\n",
      "  return cls(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 6340.txt: Latitude must be in the [-90; 90] range.\n",
      "  Skipped empty or problematic file: 6340.txt\n",
      "  Processing file 6500/10357: 6526.txt\n",
      "  Skipped empty or problematic file: 6717.txt\n",
      "  Processing file 7000/10357: 6977.txt\n",
      "  Processing file 7500/10357: 7426.txt\n",
      "  Skipped empty or problematic file: 7583.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanac\\AppData\\Local\\Temp\\ipykernel_27784\\2378111335.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['datetime'] = pd.to_datetime(df['datetime_str'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing file 8000/10357: 7877.txt\n",
      "  Skipped empty or problematic file: 8209.txt\n",
      "  Processing file 8500/10357: 8326.txt\n",
      "  Skipped empty or problematic file: 8424.txt\n",
      "  Processing file 9000/10357: 8777.txt\n",
      "  Processing file 9500/10357: 9226.txt\n",
      "  Processing file 10000/10357: 9677.txt\n",
      "  Skipped empty or problematic file: 9874.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanac\\anaconda3\\envs\\big_data\\Lib\\site-packages\\geopy\\point.py:472: UserWarning: Latitude normalization has been prohibited in the newer versions of geopy, because the normalized value happened to be on a different pole, which is probably not what was meant. If you pass coordinates as positional args, please make sure that the order is (latitude, longitude) or (y, x) in Cartesian terms.\n",
      "  return cls(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 9949.txt: Latitude must be in the [-90; 90] range.\n",
      "  Skipped empty or problematic file: 9949.txt\n",
      "  Processing file 10357/10357: 9999.txt\n",
      "\n",
      "Preprocessing complete for 10357 files.\n",
      "Successfully preprocessed and saved 10333 files to C:\\Users\\sanac\\Downloads\\Big Data\\T-drive Taxi Trajectories\\release\\taxi_preprocessed_for_kafka.\n",
      "Skipped 24 empty or problematic files.\n",
      "\n",
      "--- Data Preprocessing Workflow Complete ---\n",
      "The files in the 'taxi_preprocessed_for_kafka' directory are now ready to be used by your Kafka producer.\n"
     ]
    }
   ],
   "source": [
    "# Main Execution Flow for Preprocessing\n",
    "\n",
    "print(\"Starting data preprocessing for Kafka replay...\")\n",
    "\n",
    "# Get a list of all taxi data files\n",
    "all_taxi_files = glob.glob(os.path.join(data_directory, \"*.txt\"))\n",
    "if not all_taxi_files:\n",
    "    print(f\"ERROR: No .txt files found in {data_directory}. Please check the 'data_directory' path in Cell 2.\")\n",
    "    print(\"Workflow aborted.\")\n",
    "else:\n",
    "    print(f\"Found {len(all_taxi_files)} taxi files in {data_directory}.\")\n",
    "\n",
    "    num_files_to_process = len(all_taxi_files) # Process ALL files\n",
    "\n",
    "    print(f\"\\n--- Preprocessing {num_files_to_process} files for Kafka replay ---\")\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i, file_path in enumerate(all_taxi_files[:num_files_to_process]):\n",
    "        if (i + 1) % 500 == 0 or (i + 1) == num_files_to_process:\n",
    "            print(f\"  Processing file {i+1}/{num_files_to_process}: {os.path.basename(file_path)}\")\n",
    "\n",
    "        preprocessed_df, _ = preprocess_taxi_data_for_kafka(file_path)\n",
    "\n",
    "        if preprocessed_df is not None and not preprocessed_df.empty:\n",
    "            # Save the preprocessed DataFrame\n",
    "            output_file_name = os.path.join(preprocessed_output_dir, os.path.basename(file_path))\n",
    "            preprocessed_df.to_csv(output_file_name, index=False) # Save without pandas index\n",
    "            processed_count += 1\n",
    "        else:\n",
    "            print(f\"  Skipped empty or problematic file: {os.path.basename(file_path)}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(f\"\\nPreprocessing complete for {num_files_to_process} files.\")\n",
    "    print(f\"Successfully preprocessed and saved {processed_count} files to {preprocessed_output_dir}.\")\n",
    "    print(f\"Skipped {skipped_count} empty or problematic files.\")\n",
    "\n",
    "    print(\"\\n--- Data Preprocessing Workflow Complete ---\")\n",
    "    print(\"The files in the 'taxi_preprocessed_for_kafka' directory are now ready to be used by your Kafka producer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3e82b",
   "metadata": {},
   "source": [
    "### Data Preprocessing for Real-time Analytics\n",
    "\n",
    "Our preprocessing transforms raw taxi trajectory files into a clean, enriched format suitable for streaming into Kafka and processing by Apache Flink. This is essential because the raw T-Drive data is unstructured, noisy, and lacks the derived features needed for real-time analysis.\n",
    "\n",
    "For each individual taxi's trajectory:\n",
    "\n",
    "We begin by **loading the raw GPS points** (taxi ID, timestamp, longitude, latitude) into a Pandas DataFrame. Extracting the `taxi_id` from the filename ensures consistent and reliable identification for each taxi, providing a solid foundation for further steps.\n",
    "\n",
    "Next, we **parse the timestamp strings** into proper datetime objects. This is crucial because mathematical operations and chronological sorting cannot be performed on text, enabling accurate time difference calculations vital for understanding movement.\n",
    "\n",
    "We then **rigorously sort all GPS points chronologically**. This step is absolutely critical, as calculating features like distance and speed relies on the precise sequence of points. Without proper ordering, our insights in Flink would be nonsensical.\n",
    "\n",
    "To streamline the data flow, we **remove identical consecutive GPS records** (same time, same location). These redundant points add no analytical value and unnecessarily inflate the data volume in our Kafka stream, making processing less efficient.\n",
    "\n",
    "We then **calculate the time difference** (in minutes) between each consecutive GPS point. This `time_diff` is a core metric, essential for understanding how long a taxi took to move between locations, directly feeding into our speed calculations.\n",
    "\n",
    "Following this, we **calculate the real-world distance** (in meters) between consecutive GPS points using the Haversine formula. This accounts for the Earth's curvature, providing accurate spatial measurements necessary for determining actual travel.\n",
    "\n",
    "With time and distance in hand, we **compute the taxi's speed** in both meters per second and kilometers per hour. Logic is included to handle zero or negative time differences by setting speed to zero, preventing errors. Speed is a primary analytical insight for real-time monitoring and a crucial input for Flink's \"Calculate speed\" operator.\n",
    "\n",
    "Finally, we **flag potential outliers** rather than removing them. This critical data quality step adds boolean indicators for problematic points (e.g., unusually large time gaps, unrealistic speeds over 120 km/h, impossible distances, or exceeding a 50 km/h speed limit). This is a deliberate design choice:\n",
    "\n",
    "* **Preserving Context:** Flagging retains the original data, providing full context for downstream analysis.\n",
    "* **Flink's Responsibility:** Our Flink application is designed to make the final decisions on how to handle these flagged points in real-time. It can choose to filter them for aggregations, trigger alerts, or include them based on specific business rules.\n",
    "* **Flexibility & Debugging:** This approach allows us to adapt outlier definitions without re-processing raw data and simplifies debugging by enabling traceability of data quality issues.\n",
    "\n",
    "By performing these steps, we transform raw GPS logs into a clean, feature-rich dataset, perfectly primed for Kafka streaming and robust real-time analytics with Flink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b450d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metrics from 10333 preprocessed files...\n",
      "  Aggregating file 1000/10333\n",
      "  Aggregating file 2000/10333\n",
      "  Aggregating file 3000/10333\n",
      "  Aggregating file 4000/10333\n",
      "  Aggregating file 5000/10333\n",
      "  Aggregating file 6000/10333\n",
      "  Aggregating file 7000/10333\n",
      "  Aggregating file 8000/10333\n",
      "  Aggregating file 9000/10333\n",
      "  Aggregating file 10000/10333\n",
      "\n",
      "taxi_metrics_df created successfully.\n",
      "   taxi_id  num_points  total_time_minutes  total_distance_meters  \\\n",
      "0        1         564         8655.383333           4.414151e+05   \n",
      "1       10        5398         8885.950000           1.181000e+06   \n",
      "2      100        1272         8444.383333           1.353615e+06   \n",
      "3     1000        1693         8875.250000           1.447631e+06   \n",
      "4    10000        1558         8877.750000           6.958293e+05   \n",
      "\n",
      "   average_speed_kmph           start_time             end_time  num_outliers  \n",
      "0            3.059935  2008-02-02 15:36:08  2008-02-08 15:51:31            24  \n",
      "1            7.974390  2008-02-02 13:32:03  2008-02-08 17:38:00            62  \n",
      "2            9.617858  2008-02-02 18:44:59  2008-02-08 15:29:22             7  \n",
      "3            9.786523  2008-02-02 13:34:52  2008-02-08 17:30:07            12  \n",
      "4            4.702741  2008-02-02 13:39:48  2008-02-08 17:37:33             0  \n",
      "Taxi metrics summary saved to C:\\Users\\sanac\\Downloads\\Big Data\\T-drive Taxi Trajectories\\release\\taxi_preprocessed_for_kafka\\taxi_metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "all_preprocessed_files = glob.glob(os.path.join(preprocessed_output_dir, \"*.txt\"))\n",
    "\n",
    "# List to store metrics for each taxi\n",
    "taxi_metrics_list = []\n",
    "\n",
    "print(f\"Aggregating metrics from {len(all_preprocessed_files)} preprocessed files...\")\n",
    "\n",
    "for i, file_path in enumerate(all_preprocessed_files):\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"  Aggregating file {i+1}/{len(all_preprocessed_files)}\")\n",
    "\n",
    "    try:\n",
    "        df_preprocessed = pd.read_csv(file_path)\n",
    "        if df_preprocessed.empty:\n",
    "            continue\n",
    "\n",
    "        taxi_id = df_preprocessed['taxi_id'].iloc[0] # Get taxi_id from the DataFrame\n",
    "\n",
    "        # Calculate metrics for this taxi\n",
    "        num_points = len(df_preprocessed)\n",
    "        # For total_time_minutes, sum up the valid time_diffs\n",
    "        # Drop NaN for start/end time and count of outliers\n",
    "        total_time_minutes = df_preprocessed['time_diff'].sum()\n",
    "        total_distance_meters = df_preprocessed['distance'].sum()\n",
    "\n",
    "        # Handle potential division by zero if total_time_minutes is 0\n",
    "        average_speed_kmph = (total_distance_meters / (total_time_minutes * 60) * 3.6) if total_time_minutes > 0 else 0\n",
    "\n",
    "        # Calculate start and end time from actual datetime objects if possible, otherwise use strings\n",
    "        start_time = df_preprocessed['datetime_str'].min() if not df_preprocessed['datetime_str'].empty else None\n",
    "        end_time = df_preprocessed['datetime_str'].max() if not df_preprocessed['datetime_str'].empty else None\n",
    "\n",
    "        num_outliers = df_preprocessed['is_outlier'].sum() # Count flagged outliers\n",
    "\n",
    "        taxi_metrics_list.append({\n",
    "            'taxi_id': taxi_id,\n",
    "            'num_points': num_points,\n",
    "            'total_time_minutes': total_time_minutes,\n",
    "            'total_distance_meters': total_distance_meters,\n",
    "            'average_speed_kmph': average_speed_kmph,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'num_outliers': num_outliers\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error aggregating {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "taxi_metrics_df = pd.DataFrame(taxi_metrics_list)\n",
    "print(\"\\ntaxi_metrics_df created successfully.\")\n",
    "print(taxi_metrics_df.head())\n",
    "\n",
    "# Optional: Save the metrics DataFrame for future use\n",
    "taxi_metrics_df.to_csv(os.path.join(preprocessed_output_dir, \"taxi_metrics_summary.csv\"), index=False)\n",
    "print(f\"Taxi metrics summary saved to {os.path.join(preprocessed_output_dir, 'taxi_metrics_summary.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e65e8",
   "metadata": {},
   "source": [
    "### Purpose: Summarizing Preprocessed Taxi Data for Selection\n",
    "\n",
    "This section of the code systematically aggregates key statistics from *each* preprocessed taxi trajectory file. The primary goal is to build a comprehensive summary table (`taxi_metrics_df`) that allows us to understand the quality and characteristics of every taxi's data. This summary is then used to intelligently select the \"best\" or \"most representative\" taxi IDs for our Kafka/Flink simulation, ensuring we work with high-quality, relevant data.\n",
    "\n",
    "**Key Actions & Rationale:**\n",
    "\n",
    "* **Iterate Through Preprocessed Files:** The code loops through every single preprocessed `.txt` file, loading each one into a DataFrame. This ensures we capture metrics for all taxis.\n",
    "* **Calculate Per-Taxi Metrics:** For each taxi, it calculates essential summary statistics directly from the preprocessed data:\n",
    "    * **Number of Points:** Indicates data density and duration of the recorded trajectory.\n",
    "    * **Total Time & Distance:** Sums up the calculated `time_diff` and `distance` values to give a total duration and total distance traveled for the entire trip, crucial for understanding activity.\n",
    "    * **Average Speed:** Computes the overall average speed for the taxi, providing a single metric to gauge its general movement behavior.\n",
    "    * **Start/End Times:** Captures the span of the recorded journey.\n",
    "    * **Outlier Count:** Sums the `is_outlier` flags to quantify the data quality, indicating how many potentially problematic points were found in that taxi's trajectory.\n",
    "* **Store Metrics:** Each taxi's calculated metrics are stored as a row in a growing list, eventually forming the `taxi_metrics_df`.\n",
    "* **Generate Final Summary Table:** Converts the collected metrics into a structured Pandas DataFrame. This format is ideal for filtering, sorting, and analyzing the overall characteristics of your taxi fleet's data.\n",
    "* **Persist Summary to CSV:** The `taxi_metrics_df` is saved to `taxi_metrics_summary.csv`. This is vital for reproducibility; it means you don't need to re-run the lengthy aggregation process every time you want to review taxi characteristics or refine your selection criteria. It acts as a persistent snapshot of your dataset's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0e7990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded taxi_metrics_df from: C:\\Users\\sanac\\Downloads\\Big Data\\T-drive Taxi Trajectories\\release\\taxi_preprocessed_for_kafka\\taxi_metrics_summary.csv\n",
      "\n",
      "Original taxi_metrics_df head:\n",
      "   taxi_id  num_points  total_time_minutes  total_distance_meters  \\\n",
      "0        1         564         8655.383333           4.414151e+05   \n",
      "1       10        5398         8885.950000           1.181000e+06   \n",
      "2      100        1272         8444.383333           1.353615e+06   \n",
      "3     1000        1693         8875.250000           1.447631e+06   \n",
      "4    10000        1558         8877.750000           6.958293e+05   \n",
      "\n",
      "   average_speed_kmph           start_time             end_time  num_outliers  \n",
      "0            3.059935  2008-02-02 15:36:08  2008-02-08 15:51:31            24  \n",
      "1            7.974390  2008-02-02 13:32:03  2008-02-08 17:38:00            62  \n",
      "2            9.617858  2008-02-02 18:44:59  2008-02-08 15:29:22             7  \n",
      "3            9.786523  2008-02-02 13:34:52  2008-02-08 17:30:07            12  \n",
      "4            4.702741  2008-02-02 13:39:48  2008-02-08 17:37:33             0  \n",
      "Total taxis in summary: 10333\n",
      "\n",
      "Applying selection filters...\n",
      "\n",
      "Filtered taxis meeting all criteria: 119 out of 10333 original taxis.\n",
      "\n",
      "Selected Taxis (top 10 based on criteria and sorted by total distance):\n",
      "|   taxi_id |   num_points |   total_time_minutes |   total_distance_meters |   average_speed_kmph | start_time          | end_time            |   num_outliers |\n",
      "|----------:|-------------:|---------------------:|------------------------:|---------------------:|:--------------------|:--------------------|---------------:|\n",
      "|      4867 |         9324 |              8888    |             5.88452e+06 |              39.7245 | 2008-02-02 13:31:10 | 2008-02-08 17:39:10 |            144 |\n",
      "|      5414 |         6837 |              8767.8  |             5.80351e+06 |              39.7147 | 2008-02-02 13:32:01 | 2008-02-08 15:39:49 |             17 |\n",
      "|      4401 |         9380 |              8886.93 |             5.71104e+06 |              38.558  | 2008-02-02 13:31:44 | 2008-02-08 17:38:40 |             51 |\n",
      "|      1971 |         5806 |              8700.13 |             5.5624e+06  |              38.3608 | 2008-02-02 13:31:12 | 2008-02-08 14:31:20 |             71 |\n",
      "|      4854 |         6964 |              8886.33 |             5.5227e+06  |              37.2889 | 2008-02-02 13:31:28 | 2008-02-08 17:37:48 |            108 |\n",
      "|      5854 |         8683 |              8875.07 |             5.51259e+06 |              37.268  | 2008-02-02 13:40:28 | 2008-02-08 17:35:32 |             84 |\n",
      "|      1948 |         6539 |              8883.15 |             5.48292e+06 |              37.0336 | 2008-02-02 13:32:42 | 2008-02-08 17:35:51 |             60 |\n",
      "|       534 |        32782 |              8888    |             5.47985e+06 |              36.9927 | 2008-02-02 13:31:06 | 2008-02-08 17:39:06 |             66 |\n",
      "|      5460 |         8538 |              8882.3  |             5.38846e+06 |              36.3991 | 2008-02-02 13:35:35 | 2008-02-08 17:37:53 |             25 |\n",
      "|      5316 |        10064 |              8755.25 |             5.33282e+06 |              36.546  | 2008-02-02 13:31:28 | 2008-02-08 15:26:43 |             49 |\n"
     ]
    }
   ],
   "source": [
    "#Select \"Best\" Taxi IDs \n",
    "\n",
    "taxi_metrics_summary_path = os.path.join(preprocessed_output_dir, \"taxi_metrics_summary.csv\")\n",
    "\n",
    "try:\n",
    "    taxi_metrics_df = pd.read_csv(taxi_metrics_summary_path)\n",
    "    print(f\"Loaded taxi_metrics_df from: {taxi_metrics_summary_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: taxi_metrics_summary.csv not found at {taxi_metrics_summary_path}.\")\n",
    "    print(\"Please ensure you have run the preprocessing and metrics aggregation step to generate this file.\")\n",
    "    # In a real script, you might sys.exit() here\n",
    "    raise # Or raise an error to stop execution if in a script\n",
    "\n",
    "print(\"\\nOriginal taxi_metrics_df head:\")\n",
    "print(taxi_metrics_df.head())\n",
    "print(f\"Total taxis in summary: {len(taxi_metrics_df)}\")\n",
    "\n",
    "# --- Define Selection Criteria ---\n",
    "MIN_POINTS = 5000\n",
    "MIN_AVG_SPEED_KMPH = 10\n",
    "MAX_AVG_SPEED_KMPH = 40\n",
    "MAX_OUTLIER_PROPORTION = 0.02\n",
    "MIN_TOTAL_DISTANCE_KM = 50\n",
    "MIN_TOTAL_TIME_HOURS = 10\n",
    "\n",
    "# --- Apply Filters ---\n",
    "print(\"\\nApplying selection filters...\")\n",
    "filtered_taxis_df = taxi_metrics_df[\n",
    "    (taxi_metrics_df['num_points'] >= MIN_POINTS) &\n",
    "    (taxi_metrics_df['average_speed_kmph'] >= MIN_AVG_SPEED_KMPH) &\n",
    "    (taxi_metrics_df['average_speed_kmph'] <= MAX_AVG_SPEED_KMPH) &\n",
    "    (taxi_metrics_df['num_outliers'] / taxi_metrics_df['num_points'] <= MAX_OUTLIER_PROPORTION) &\n",
    "    (taxi_metrics_df['total_distance_meters'] / 1000 >= MIN_TOTAL_DISTANCE_KM) &\n",
    "    (taxi_metrics_df['total_time_minutes'] / 60 >= MIN_TOTAL_TIME_HOURS)\n",
    "].copy()\n",
    "\n",
    "filtered_taxis_df = filtered_taxis_df.sort_values(by='total_distance_meters', ascending=False)\n",
    "\n",
    "print(f\"\\nFiltered taxis meeting all criteria: {len(filtered_taxis_df)} out of {len(taxi_metrics_df)} original taxis.\")\n",
    "print(\"\\nSelected Taxis (top 10 based on criteria and sorted by total distance):\")\n",
    "print(filtered_taxis_df.head(10).to_markdown(index=False))\n",
    "\n",
    "# Populate selected_taxi_ids for the next block\n",
    "selected_taxi_ids = filtered_taxis_df['taxi_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc97a2a",
   "metadata": {},
   "source": [
    "### Rationale for Taxi Selection Criteria\n",
    "\n",
    "The specific values chosen for filtering taxis are based on a combination of data exploration and project goals:\n",
    "\n",
    "* **`MIN_POINTS = 5000` (Number of GPS Points):** Ensures selected taxis have a **sufficiently long and detailed trajectory**, representing substantial activity, not just short, incomplete trips.\n",
    "* **`MIN_AVG_SPEED_KMPH = 10` & `MAX_AVG_SPEED_KMPH = 40` (Average Speed):**\n",
    "    * **10 km/h (Min):** Filters out taxis that were predominantly **stationary or had negligible average movement**, indicating idle time or noisy data over the entire period. Focuses on active taxis.\n",
    "    * **40 km/h (Max):** Excludes taxis with **unrealistically high average speeds for urban operations**, which might signify data errors or non-representative highway travel not relevant to urban taxi patterns.\n",
    "* **`MAX_OUTLIER_PROPORTION = 0.02` (Outlier Proportion):** Limits chosen taxis to those with **minimal data quality issues** (e.g., extreme speeds, time gaps, distances). A 2% threshold helps ensure the selected data is cleaner and more reliable for streaming.\n",
    "* **`MIN_TOTAL_DISTANCE_KM = 50` (Total Distance):** Selects taxis that have **covered a meaningful distance**, confirming they were actively used for travel rather than just static logging.\n",
    "* **`MIN_TOTAL_TIME_HOURS = 10` (Total Time):** Ensures taxis have **recorded data over a significant duration**, providing rich, continuous activity patterns suitable for real-time simulation.\n",
    "\n",
    "These criteria collectively define a \"high-quality\" and \"representative\" taxi trajectory subset, allowing us to focus our Kafka/Flink simulation on data that best reflects typical and reliable urban taxi operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9831577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Further limiting the final selection to top 10 taxis.\n",
      "\n",
      "Final count of taxi IDs to use: 10\n",
      "\n",
      "--- Details for Final Selected Taxis ---\n",
      "|   taxi_id |   num_points |   total_time_minutes |   total_distance_meters |   average_speed_kmph | start_time          | end_time            |   num_outliers |\n",
      "|----------:|-------------:|---------------------:|------------------------:|---------------------:|:--------------------|:--------------------|---------------:|\n",
      "|      4867 |         9324 |              8888    |             5.88452e+06 |              39.7245 | 2008-02-02 13:31:10 | 2008-02-08 17:39:10 |            144 |\n",
      "|      5414 |         6837 |              8767.8  |             5.80351e+06 |              39.7147 | 2008-02-02 13:32:01 | 2008-02-08 15:39:49 |             17 |\n",
      "|      4401 |         9380 |              8886.93 |             5.71104e+06 |              38.558  | 2008-02-02 13:31:44 | 2008-02-08 17:38:40 |             51 |\n",
      "|      1971 |         5806 |              8700.13 |             5.5624e+06  |              38.3608 | 2008-02-02 13:31:12 | 2008-02-08 14:31:20 |             71 |\n",
      "|      4854 |         6964 |              8886.33 |             5.5227e+06  |              37.2889 | 2008-02-02 13:31:28 | 2008-02-08 17:37:48 |            108 |\n",
      "|      5854 |         8683 |              8875.07 |             5.51259e+06 |              37.268  | 2008-02-02 13:40:28 | 2008-02-08 17:35:32 |             84 |\n",
      "|      1948 |         6539 |              8883.15 |             5.48292e+06 |              37.0336 | 2008-02-02 13:32:42 | 2008-02-08 17:35:51 |             60 |\n",
      "|       534 |        32782 |              8888    |             5.47985e+06 |              36.9927 | 2008-02-02 13:31:06 | 2008-02-08 17:39:06 |             66 |\n",
      "|      5460 |         8538 |              8882.3  |             5.38846e+06 |              36.3991 | 2008-02-02 13:35:35 | 2008-02-08 17:37:53 |             25 |\n",
      "|      5316 |        10064 |              8755.25 |             5.33282e+06 |              36.546  | 2008-02-02 13:31:28 | 2008-02-08 15:26:43 |             49 |\n",
      "List of selected taxi IDs saved to C:\\Users\\sanac\\Downloads\\Big Data\\T-drive Taxi Trajectories\\release\\taxi_preprocessed_for_kafka\\selected_taxi_ids.txt\n",
      "\n",
      "--- Taxi ID Selection Workflow Complete ---\n",
      "You now have a 'selected_taxi_ids.txt' file that your Kafka producer can use.\n"
     ]
    }
   ],
   "source": [
    "# --- Further limit the number of selected taxis if needed for your environment ---\n",
    "MAX_SELECTED_TAXI_FILES_FOR_YOUR_RUN = 10 # Example: only use the top 10 best taxis for your Flink testing\n",
    "\n",
    "if len(selected_taxi_ids) > MAX_SELECTED_TAXI_FILES_FOR_YOUR_RUN:\n",
    "    final_selected_taxi_ids = selected_taxi_ids[:MAX_SELECTED_TAXI_FILES_FOR_YOUR_RUN]\n",
    "    print(f\"\\nFurther limiting the final selection to top {MAX_SELECTED_TAXI_FILES_FOR_YOUR_RUN} taxis.\")\n",
    "else:\n",
    "    final_selected_taxi_ids = selected_taxi_ids\n",
    "    print(f\"\\nUsing all {len(final_selected_taxi_ids)} selected taxis (no further limit applied).\")\n",
    "\n",
    "print(f\"\\nFinal count of taxi IDs to use: {len(final_selected_taxi_ids)}\")\n",
    "\n",
 
    "print(\"\\n--- Details for Final Selected Taxis ---\")\n",
    "# Filter the original filtered_taxis_df (which is already sorted and clean)\n",
    "# to only include the final_selected_taxi_ids.\n",
    "# Using isin() is efficient for matching multiple IDs.\n",
    "final_selected_taxis_details_df = filtered_taxis_df[\n",
    "    filtered_taxis_df['taxi_id'].isin(final_selected_taxi_ids)\n",
    "].copy()\n",
    "\n",
    "# Print the full details in a nice Markdown table format\n",
    "print(final_selected_taxis_details_df.to_markdown(index=False))\n",
    
    "\n",
    "\n",
    "# Save the list of selected taxi IDs to a file. Your Kafka producer will read this.\n",
    "# Ensure preprocessed_output_dir is accessible/defined if running in a new session or script\n",
    "with open(os.path.join(preprocessed_output_dir, \"selected_taxi_ids.txt\"), \"w\") as f:\n",
    "    for taxi_id in final_selected_taxi_ids:\n",
    "        f.write(f\"{taxi_id}\\n\")\n",
    "\n",
    "print(f\"List of selected taxi IDs saved to {os.path.join(preprocessed_output_dir, 'selected_taxi_ids.txt')}\")\n",
    "\n",
    "print(\"\\n--- Taxi ID Selection Workflow Complete ---\")\n",
    "print(\"You now have a 'selected_taxi_ids.txt' file that your Kafka producer can use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17be39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Details for Final Selected Taxis ---\n",
      "|   taxi_id |   num_points | total_time_minutes   | total_distance_meters   |   average_speed_kmph | start_time          | end_time            |   num_outliers |\n",
      "|----------:|-------------:|:---------------------|:------------------------|---------------------:|:--------------------|:--------------------|---------------:|\n",
      "|      4867 |         9324 | 8,888.00             | 5,884,518.43            |                39.72 | 2008-02-02 13:31:10 | 2008-02-08 17:39:10 |            144 |\n",
      "|      5414 |         6837 | 8,767.80             | 5,803,508.12            |                39.71 | 2008-02-02 13:32:01 | 2008-02-08 15:39:49 |             17 |\n",
      "|      4401 |         9380 | 8,886.93             | 5,711,040.58            |                38.56 | 2008-02-02 13:31:44 | 2008-02-08 17:38:40 |             51 |\n",
      "|      1971 |         5806 | 8,700.13             | 5,562,400.83            |                38.36 | 2008-02-02 13:31:12 | 2008-02-08 14:31:20 |             71 |\n",
      "|      4854 |         6964 | 8,886.33             | 5,522,699.26            |                37.29 | 2008-02-02 13:31:28 | 2008-02-08 17:37:48 |            108 |\n",
      "|      5854 |         8683 | 8,875.07             | 5,512,592.78            |                37.27 | 2008-02-02 13:40:28 | 2008-02-08 17:35:32 |             84 |\n",
      "|      1948 |         6539 | 8,883.15             | 5,482,919.94            |                37.03 | 2008-02-02 13:32:42 | 2008-02-08 17:35:51 |             60 |\n",
      "|       534 |        32782 | 8,888.00             | 5,479,845.33            |                36.99 | 2008-02-02 13:31:06 | 2008-02-08 17:39:06 |             66 |\n",
      "|      5460 |         8538 | 8,882.30             | 5,388,464.66            |                36.4  | 2008-02-02 13:35:35 | 2008-02-08 17:37:53 |             25 |\n",
      "|      5316 |        10064 | 8,755.25             | 5,332,821.74            |                36.55 | 2008-02-02 13:31:28 | 2008-02-08 15:26:43 |             49 |\n"
     ]
    }
   ],
   "source": [
    "# --- ADDED CODE BELOW ---\n",
    "print(\"\\n--- Details for Final Selected Taxis ---\")\n",
    "\n",
    "final_selected_taxis_details_df = filtered_taxis_df[\n",
    "    filtered_taxis_df['taxi_id'].isin(final_selected_taxi_ids)\n",
    "].copy()\n",
    "\n",
    "# --- Apply number formatting for display purposes ---\n",
    "# Create a dictionary of formatters for specific columns\n",
    "formatters = {\n",
    "    'total_distance_meters': '{:,.2f}'.format,  # Format as float with 2 decimal places and comma separator\n",
    "    'total_time_minutes': '{:,.2f}'.format,    # Same for total_time_minutes\n",
    "    'average_speed_kmph': '{:,.2f}'.format     # Same for average_speed_kmph\n",
    "    # You can add more formatters for other columns if needed\n",
    "}\n",
    "\n",
    "# Create a display copy to apply formatting without altering the original DataFrame\n",
    "display_df = final_selected_taxis_details_df.copy()\n",
    "\n",
    "# Apply the formatting directly to the columns for display\n",
    "display_df['total_distance_meters'] = display_df['total_distance_meters'].apply(lambda x: f'{x:,.2f}')\n",
    "display_df['total_time_minutes'] = display_df['total_time_minutes'].apply(lambda x: f'{x:,.2f}')\n",
    "display_df['average_speed_kmph'] = display_df['average_speed_kmph'].apply(lambda x: f'{x:,.2f}')\n",
    "\n",
    "\n",
    "print(display_df.to_markdown(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
