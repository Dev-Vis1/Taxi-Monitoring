services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.1 
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: unless-stopped
    networks:
      - taxi_network

  kafka:
    image: confluentinc/cp-kafka:6.1.1
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 50
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MESSAGE_MAX_BYTES: 10485760
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 1048576
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_LOG_RETENTION_HOURS: 12
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_COMPRESSION_TYPE: lz4
      KAFKA_BATCH_SIZE: 500000
      KAFKA_LINGER_MS: 10
      KAFKA_BUFFER_MEMORY: 67108864
      KAFKA_ACKS: "0"
    restart: unless-stopped
    networks:
      - taxi_network
    deploy:
      resources:
        limits:
          memory: 6G          # Increased for high throughput
          cpus: "3.0"         # More CPU for processing

  python-producer:
    build:
      context: ./taxi_locations/producer
      dockerfile: Dockerfile
    volumes:
      - ./taxi_locations:/app
    depends_on:
      - kafka
    environment:
      KAFKA_BROKER: kafka:29092
      MAX_WORKERS: 16          # Increased for maximum throughput
    restart: unless-stopped
    networks:
      - taxi_network
    deploy:
      resources:
        limits:
          memory: 4G          # Increased for processing large dataset
          cpus: "4.0"         # Max CPU for parallel processing

  redis:
    image: redis:7.0-alpine
    container_name: redis
    ports:
      - "6379:6379"
    command: >
      redis-server
      --save ""
      --appendonly no
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --timeout 300
      --tcp-keepalive 60
    restart: unless-stopped
    networks:
      - taxi_network
    deploy:
      resources:
        limits:
          memory: 2G          # Reduced from 4G for efficiency
          cpus: "1.0"         # Reduced from 2.0

  dash-app:
    build:
      context: ./dash-app
      dockerfile: Dockerfile
    container_name: dash-app
    ports:
      - "8050:8050"
    depends_on:
      - redis
    networks:
      - taxi_network
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "1.5"
  
  flink-jobmanager:
    image: flink:1.18.1-scala_2.12-java17
    container_name: flink-jobmanager
    ports:
      - "8081:8081"
    volumes:
      - ./flink-checkpoints:/checkpoints
      - ./taxi_locations/consumer/taxi_flink/target:/opt/flink/usrlib
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.memory.process.size: 3072mb
        state.backend: rocksdb
        state.checkpoints.dir: file:///checkpoints
        state.backend.rocksdb.memory.managed: true
        state.backend.rocksdb.memory.fixed-per-slot: 128mb
        execution.checkpointing.interval: 180s
        execution.checkpointing.timeout: 8min
        state.backend.rocksdb.block.cache-size: 32MB
        state.backend.rocksdb.writebuffer.size: 16MB
        env.java.opts: "-XX:MaxDirectMemorySize=1g -XX:NativeMemoryTracking=summary"
    command: jobmanager
    restart: unless-stopped
    networks:
      - taxi_network
    deploy:
      resources:
        limits:
          memory: 3G          # Reduced from 6G
          cpus: "1.5"         # Reduced from 2.0
  
  flink-taskmanager:
    image: flink:1.18.1-scala_2.12-java17
    depends_on:
      - flink-jobmanager
    volumes:
      - ./flink-checkpoints:/checkpoints
      - ./taxi_locations/consumer/taxi_flink/target:/opt/flink/usrlib
    command: taskmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 8
        taskmanager.memory.process.size: 6144mb
        taskmanager.memory.managed.fraction: 0.7
        state.backend.rocksdb.writebuffer.count: 4
        state.backend.rocksdb.thread.num: 4
        state.backend.rocksdb.block.cache-size: 64MB
        state.backend.rocksdb.writebuffer.size: 32MB
        env.java.opts: "-XX:MaxDirectMemorySize=3g -XX:NativeMemoryTracking=summary -XX:+UseG1GC"
    restart: unless-stopped
    networks:
      - taxi_network
    deploy:
      resources:
        limits:
          memory: 7G          # Increased to match new memory settings
          cpus: "6.0"         # Increased to handle 8 slots efficiently
      replicas: 1

networks:
  taxi_network:
    driver: bridge